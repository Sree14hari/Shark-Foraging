{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed91aada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "Data loaded and cleaned.\n",
      "Calculating all features...\n",
      "All features calculated successfully.\n",
      "\n",
      "Data preparation complete!\n",
      "New DataFrame includes 'id' and 'date' for sequencing:\n",
      "                   id                date        lat        lon  month  hour  \\\n",
      "3  160424_2013_132346 2013-07-29 16:38:58  40.605000 -71.742004      7    16   \n",
      "4  160424_2013_132346 2013-07-29 16:49:22  40.548000 -71.621002      7    16   \n",
      "5  160424_2013_132346 2013-07-29 17:39:59  40.544998 -71.540009      7    17   \n",
      "6  160424_2013_132346 2013-07-29 19:18:58  40.513000 -71.506989      7    19   \n",
      "7  160424_2013_132346 2013-07-29 19:53:41  40.516998 -71.585999      7    19   \n",
      "\n",
      "   speed_avg_roll  speed_std_roll  angle_avg_roll  angle_std_roll  is_foraging  \n",
      "3        1.468594        1.743751      160.152236      106.012042            0  \n",
      "4        5.919279        9.014518      127.892000       93.495695            0  \n",
      "5        5.186637        7.976840      103.169286       90.953177            0  \n",
      "6        5.239584        7.938714       92.358160       82.393947            0  \n",
      "7        5.796812        7.607366       71.726039       39.418163            0  \n",
      "\n",
      "✅ 'processed_shark_data.csv' has been created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreeh\\AppData\\Local\\Temp\\ipykernel_40948\\3355567507.py:17: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
      "C:\\Users\\sreeh\\AppData\\Local\\Temp\\ipykernel_40948\\3355567507.py:58: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('id').apply(create_all_features)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import great_circle\n",
    "import glob\n",
    "\n",
    "# --- Steps 1 & 2: Load, Clean, and Sort ---\n",
    "print(\"Loading and cleaning data...\")\n",
    "try:\n",
    "    csv_files = glob.glob('Dataset/*.csv')\n",
    "    if not csv_files:\n",
    "        print(\"Error: No CSV files found in the current directory.\")\n",
    "    else:\n",
    "        df_list = [pd.read_csv(file, header=None, names=['id', 'date', 'lc', 'lon', 'lat']) for file in csv_files]\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "        df.dropna(subset=['lat', 'lon'], inplace=True)\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df.dropna(subset=['date'], inplace=True)\n",
    "        df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
    "        df['lon'] = pd.to_numeric(df['lon'], errors='coerce')\n",
    "        df.dropna(subset=['lat', 'lon'], inplace=True)\n",
    "        df.sort_values(by=['id', 'date'], inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        print(\"Data loaded and cleaned.\")\n",
    "\n",
    "        # --- Combined Feature Creation Function ---\n",
    "        def create_all_features(group):\n",
    "            prev_lat = group['lat'].shift(1)\n",
    "            prev_lon = group['lon'].shift(1)\n",
    "            prev_date = group['date'].shift(1)\n",
    "            \n",
    "            distances = [great_circle((lat, lon), (prev_lat, prev_lon)).meters if not pd.isna(prev_lat) else np.nan\n",
    "                         for lat, lon, prev_lat, prev_lon in zip(group['lat'], group['lon'], prev_lat, prev_lon)]\n",
    "            group['distance_m'] = distances\n",
    "            \n",
    "            time_diff_s = (group['date'] - prev_date).dt.total_seconds()\n",
    "            group['speed_mps'] = group['distance_m'] / time_diff_s.replace(0, np.nan)\n",
    "\n",
    "            lat1, lon1 = np.radians(prev_lat), np.radians(prev_lon)\n",
    "            lat2, lon2 = np.radians(group['lat']), np.radians(group['lon'])\n",
    "            dLon = lon2 - lon1\n",
    "            y = np.sin(dLon) * np.cos(lat2)\n",
    "            x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(dLon)\n",
    "            bearing = np.degrees(np.arctan2(y, x))\n",
    "            group['bearing'] = (bearing + 360) % 360\n",
    "            group['turning_angle'] = abs(group['bearing'].diff())\n",
    "\n",
    "            window_size = 5\n",
    "            group['speed_avg_roll'] = group['speed_mps'].rolling(window=window_size, min_periods=1).mean()\n",
    "            group['speed_std_roll'] = group['speed_mps'].rolling(window=window_size, min_periods=1).std()\n",
    "            group['angle_avg_roll'] = group['turning_angle'].rolling(window=window_size, min_periods=1).mean()\n",
    "            group['angle_std_roll'] = group['turning_angle'].rolling(window=window_size, min_periods=1).std()\n",
    "            \n",
    "            return group\n",
    "\n",
    "        # --- Apply the single function ---\n",
    "        print(\"Calculating all features...\")\n",
    "        df = df.groupby('id').apply(create_all_features)\n",
    "        print(\"All features calculated successfully.\")\n",
    "\n",
    "        # --- Define Target and Create Final DataFrame ---\n",
    "        speed_threshold = 0.5 \n",
    "        angle_threshold = 20\n",
    "        df['is_foraging'] = ((df['speed_mps'] < speed_threshold) & (df['turning_angle'] > angle_threshold)).astype(int)\n",
    "\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['hour'] = df['date'].dt.hour\n",
    "\n",
    "        # Define final columns\n",
    "        final_columns = [\n",
    "            'id',\n",
    "            'date',\n",
    "            'lat', 'lon', 'month', 'hour', \n",
    "            'speed_avg_roll', 'speed_std_roll', 'angle_avg_roll', 'angle_std_roll',\n",
    "            'is_foraging'\n",
    "        ]\n",
    "        \n",
    "        # --- THE FIX IS HERE ---\n",
    "        # First, drop the old 'id' column which is now redundant\n",
    "        if 'id' in df.columns:\n",
    "            df = df.drop('id', axis=1)\n",
    "        \n",
    "        # Now, reset the index to turn the 'id' from the index back into a column\n",
    "        df.reset_index(inplace=True)\n",
    "\n",
    "        final_df = df.dropna(subset=final_columns)\n",
    "        final_df = final_df[final_columns]\n",
    "\n",
    "        print(\"\\nData preparation complete!\")\n",
    "        print(\"New DataFrame includes 'id' and 'date' for sequencing:\")\n",
    "        print(final_df.head())\n",
    "\n",
    "        final_df.to_csv('processed_shark_data.csv', index=False)\n",
    "        print(\"\\n✅ 'processed_shark_data.csv' has been created successfully.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b85c2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the saved model and scaler...\n",
      "Model and scaler loaded successfully.\n",
      "Loading coastline data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreeh\\AppData\\Local\\Temp\\ipykernel_40948\\3800766022.py:17: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
      "  land_polygon = land_gdf.unary_union\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coastline data loaded.\n",
      "Creating prediction grid...\n",
      "Filtering grid points... Starting with 1600 points.\n",
      "Finished filtering. 996 ocean points remaining.\n",
      "Engineering features for the grid...\n",
      "Scaling grid features...\n",
      "Predicting foraging probability on the grid...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a:\\SHARKS\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction complete.\n",
      "Exporting results to hotspots.geojson...\n",
      "\n",
      "✅ Operation Complete! Your final 'hotspots.geojson' now only contains ocean points.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import geojson\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# 1. Load your best model and the scaler\n",
    "print(\"Loading the saved model and scaler...\")\n",
    "model = joblib.load('best_shark_model_lightgbm.pkl')\n",
    "scaler = joblib.load('shark_model_scaler.pkl')\n",
    "print(\"Model and scaler loaded successfully.\")\n",
    "\n",
    "# Load the land shapefile\n",
    "print(\"Loading coastline data...\")\n",
    "land_gdf = gpd.read_file('Land/ne_50m_land.shp')\n",
    "land_polygon = land_gdf.unary_union\n",
    "print(\"Coastline data loaded.\")\n",
    "\n",
    "# 2. Create a Grid for the North Atlantic\n",
    "print(\"Creating prediction grid...\")\n",
    "min_lon, max_lon = -80, -60\n",
    "min_lat, max_lat = 30, 50\n",
    "grid_resolution = 0.5\n",
    "\n",
    "lons = np.arange(min_lon, max_lon, grid_resolution)\n",
    "lats = np.arange(min_lat, max_lat, grid_resolution)\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "\n",
    "grid_df = pd.DataFrame({\n",
    "    'lon': lon_grid.flatten(),\n",
    "    'lat': lat_grid.flatten()\n",
    "})\n",
    "\n",
    "# --- CORRECTED FILTERING LOGIC ---\n",
    "print(f\"Filtering grid points... Starting with {len(grid_df)} points.\")\n",
    "# Create a GeoDataFrame from our grid points\n",
    "geometry = [Point(xy) for xy in zip(grid_df['lon'], grid_df['lat'])]\n",
    "grid_gdf = gpd.GeoDataFrame(grid_df, geometry=geometry)\n",
    "\n",
    "# Filter the GeoDataFrame to keep only points NOT on land\n",
    "ocean_gdf = grid_gdf[~grid_gdf.geometry.within(land_polygon)]\n",
    "\n",
    "# Create a new, clean DataFrame from the filtered ocean points, dropping the geometry\n",
    "grid_df = pd.DataFrame(ocean_gdf.drop(columns='geometry'))\n",
    "print(f\"Finished filtering. {len(grid_df)} ocean points remaining.\")\n",
    "# --- END OF FIX ---\n",
    "\n",
    "# 3. Engineer features for the remaining ocean points\n",
    "print(\"Engineering features for the grid...\")\n",
    "month = 8\n",
    "hour = 22\n",
    "\n",
    "grid_df['month'] = month\n",
    "grid_df['hour'] = hour\n",
    "grid_df['hour_sin'] = np.sin(2 * np.pi * hour/24.0)\n",
    "grid_df['hour_cos'] = np.cos(2 * np.pi * hour/24.0)\n",
    "grid_df['month_sin'] = np.sin(2 * np.pi * month/12.0)\n",
    "grid_df['month_cos'] = np.cos(2 * np.pi * month/12.0)\n",
    "\n",
    "placeholder_features = [\n",
    "    'speed_avg_roll', 'speed_std_roll', 'angle_avg_roll', 'angle_std_roll',\n",
    "    'speed_percentile', 'is_slow', 'speed_change', 'high_turn_angle',\n",
    "    'angle_consistency', 'speed_angle_ratio', 'movement_efficiency',\n",
    "    'turning_intensity', 'lat_rounded', 'lon_rounded', 'distance_from_center',\n",
    "    'dawn_dusk', 'night', 'speed_z_score', 'angle_z_score'\n",
    "]\n",
    "for feat in placeholder_features:\n",
    "    grid_df[feat] = 0\n",
    "\n",
    "grid_df['lat_rounded'] = np.round(grid_df['lat'], 2)\n",
    "grid_df['lon_rounded'] = np.round(grid_df['lon'], 2)\n",
    "grid_df['distance_from_center'] = np.sqrt((grid_df['lat'] - 40)**2 + (grid_df['lon'] - (-70))**2)\n",
    "\n",
    "# 4. Scale and Predict\n",
    "print(\"Scaling grid features...\")\n",
    "X_train_columns = scaler.feature_names_in_\n",
    "grid_df = grid_df[X_train_columns]\n",
    "grid_scaled = scaler.transform(grid_df)\n",
    "\n",
    "print(\"Predicting foraging probability on the grid...\")\n",
    "probabilities = model.predict_proba(grid_scaled)[:, 1]\n",
    "grid_df['probability'] = probabilities\n",
    "print(\"Prediction complete.\")\n",
    "\n",
    "# 5. Export to GeoJSON\n",
    "print(\"Exporting results to hotspots.geojson...\")\n",
    "features = []\n",
    "for i, row in grid_df.iterrows():\n",
    "    if row['probability'] > 0.5:\n",
    "        point = geojson.Point((row['lon'], row['lat']))\n",
    "        feature = geojson.Feature(geometry=point, properties={'probability': round(float(row['probability']), 4)})\n",
    "        features.append(feature)\n",
    "\n",
    "feature_collection = geojson.FeatureCollection(features)\n",
    "with open('hotspots.geojson', 'w') as f:\n",
    "    geojson.dump(feature_collection, f)\n",
    "\n",
    "print(\"\\n✅ Operation Complete! Your final 'hotspots.geojson' now only contains ocean points.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
