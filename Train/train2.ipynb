{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "558b519c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (9149, 11)\n",
      "Target distribution:\n",
      "is_foraging\n",
      "0    6417\n",
      "1    2732\n",
      "Name: count, dtype: int64\n",
      "Enhanced dataset shape: (9149, 28)\n",
      "\n",
      "Applying SMOTE to training set...\n",
      "After resample: [5133 5133]\n",
      "\n",
      "Optimizing hyperparameters for LightGBM...\n",
      "Best LightGBM params: {'subsample': 0.8, 'reg_lambda': 0.5, 'reg_alpha': 0.1, 'num_leaves': 100, 'n_estimators': 500, 'min_child_samples': 20, 'max_depth': 8, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "\n",
      "============================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "LightGBM Accuracy: 0.7634\n",
      "Random Forest Accuracy: 0.7497\n",
      "Ensemble Accuracy: 0.7612\n",
      "\n",
      "Best Model: LightGBM\n",
      "Best Accuracy: 0.7634 (76.34%)\n",
      "\n",
      "Classification Report (LightGBM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Traveling       0.83      0.83      0.83      1284\n",
      "    Foraging       0.61      0.60      0.60       546\n",
      "\n",
      "    accuracy                           0.76      1830\n",
      "   macro avg       0.72      0.72      0.72      1830\n",
      "weighted avg       0.76      0.76      0.76      1830\n",
      "\n",
      "\n",
      "Confusion Matrix (LightGBM):\n",
      "[[1072  212]\n",
      " [ 221  325]]\n",
      "\n",
      "Top 10 Features:\n",
      "                 feature  importance\n",
      "14          speed_change        3093\n",
      "16     angle_consistency        2301\n",
      "18   movement_efficiency        2286\n",
      "7         angle_std_roll        2043\n",
      "6         angle_avg_roll        1986\n",
      "0                    lat        1965\n",
      "19     turning_intensity        1964\n",
      "12      speed_percentile        1951\n",
      "20  distance_from_center        1863\n",
      "17     speed_angle_ratio        1827\n",
      "\n",
      "Best model saved as best_shark_model_lightgbm.pkl\n",
      "Scaler saved as shark_model_scaler.pkl\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if you haven't already\n",
    "# !pip install lightgbm imbalanced-learn scikit-learn pandas numpy joblib\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# DATA LOADING & FEATURE ENGINEERING\n",
    "# =============================================================================\n",
    "\n",
    "# Load the data from your latest preparation script\n",
    "df = pd.read_csv('processed_shark_data.csv')\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['is_foraging'].value_counts()}\")\n",
    "\n",
    "# 1. Cyclical time features\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour']/24.0)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour']/24.0)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month']/12.0)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month']/12.0)\n",
    "\n",
    "# 2. Advanced movement and statistical features\n",
    "# IMPROVEMENT: Grouping by shark 'id' for a more meaningful percentile.\n",
    "df['speed_percentile'] = df.groupby('id')['speed_avg_roll'].rank(pct=True)\n",
    "df['is_slow'] = (df['speed_avg_roll'] < df['speed_avg_roll'].quantile(0.25)).astype(int)\n",
    "df['speed_change'] = df.groupby('id')['speed_avg_roll'].diff().fillna(0)\n",
    "df['high_turn_angle'] = (df['angle_avg_roll'] > df['angle_avg_roll'].quantile(0.75)).astype(int)\n",
    "df['angle_consistency'] = df['angle_std_roll'] / (df['angle_avg_roll'] + 1e-6)\n",
    "df['speed_angle_ratio'] = df['speed_avg_roll'] / (df['angle_avg_roll'] + 1e-6)\n",
    "df['movement_efficiency'] = df['speed_avg_roll'] / (df['speed_std_roll'] + 1e-6)\n",
    "df['turning_intensity'] = df['angle_avg_roll'] * df['angle_std_roll']\n",
    "df['distance_from_center'] = np.sqrt((df['lat'] - df['lat'].mean())**2 + (df['lon'] - df['lon'].mean())**2)\n",
    "df['dawn_dusk'] = ((df['hour'] >= 5) & (df['hour'] <= 8) | (df['hour'] >= 17) & (df['hour'] <= 20)).astype(int)\n",
    "df['night'] = ((df['hour'] >= 21) | (df['hour'] <= 4)).astype(int)\n",
    "df['speed_z_score'] = (df['speed_avg_roll'] - df['speed_avg_roll'].mean()) / df['speed_avg_roll'].std()\n",
    "df['angle_z_score'] = (df['angle_avg_roll'] - df['angle_avg_roll'].mean()) / df['angle_avg_roll'].std()\n",
    "\n",
    "# Handle any NaN/Inf values created during feature engineering\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.fillna(df.median(numeric_only=True))\n",
    "\n",
    "print(f\"Enhanced dataset shape: {df.shape}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DATA PREPROCESSING\n",
    "# =============================================================================\n",
    "\n",
    "# THE FIX: Define features (X) by dropping the target and non-feature columns\n",
    "X = df.drop(['is_foraging', 'id', 'date'], axis=1)\n",
    "y = df['is_foraging']\n",
    "\n",
    "# Save the column order for later\n",
    "feature_columns = X.columns.tolist()\n",
    "joblib.dump(feature_columns, 'feature_columns.pkl')\n",
    "\n",
    "# Split data with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "# SAMPLING & SCALING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nApplying SMOTE to training set...\")\n",
    "# Using SMOTE as it's robust and fast\n",
    "sampler = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sampler.fit_resample(X_train, y_train)\n",
    "print(f\"After resample: {np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Apply robust scaling\n",
    "scaler = RobustScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# =============================================================================\n",
    "# HYPERPARAMETER OPTIMIZATION\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nOptimizing hyperparameters for LightGBM...\")\n",
    "lgbm_params = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [6, 8, 10],\n",
    "    'min_child_samples': [20, 30, 50],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'reg_alpha': [0, 0.1, 0.5],\n",
    "    'reg_lambda': [0, 0.1, 0.5]\n",
    "}\n",
    "lgbm_base = lgb.LGBMClassifier(random_state=42, verbose=-1)\n",
    "lgbm_random = RandomizedSearchCV(\n",
    "    lgbm_base, lgbm_params, n_iter=50, cv=5, \n",
    "    scoring='accuracy', random_state=42, n_jobs=-1\n",
    ")\n",
    "lgbm_random.fit(X_train_scaled, y_train_resampled)\n",
    "print(f\"Best LightGBM params: {lgbm_random.best_params_}\")\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL EVALUATION\n",
    "# =============================================================================\n",
    "best_lgbm = lgbm_random.best_estimator_\n",
    "\n",
    "# Additional model for comparison\n",
    "rf_model = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf_model.fit(X_train_scaled, y_train_resampled)\n",
    "\n",
    "# --- Create predictions ---\n",
    "lgbm_pred = best_lgbm.predict(X_test_scaled)\n",
    "rf_pred = rf_model.predict(X_test_scaled)\n",
    "ensemble_pred_proba = (0.7 * best_lgbm.predict_proba(X_test_scaled)[:, 1]) + (0.3 * rf_model.predict_proba(X_test_scaled)[:, 1])\n",
    "ensemble_pred = (ensemble_pred_proba > 0.5).astype(int)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Select the best model based on accuracy ---\n",
    "models = {\n",
    "    'LightGBM': lgbm_pred,\n",
    "    'Random Forest': rf_pred,\n",
    "    'Ensemble': ensemble_pred\n",
    "}\n",
    "best_model_name = max(models.keys(), key=lambda k: accuracy_score(y_test, models[k]))\n",
    "best_accuracy = accuracy_score(y_test, models[best_model_name])\n",
    "\n",
    "print(f\"\\nLightGBM Accuracy: {accuracy_score(y_test, lgbm_pred):.4f}\")\n",
    "print(f\"Random Forest Accuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
    "print(f\"Ensemble Accuracy: {accuracy_score(y_test, ensemble_pred):.4f}\")\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n",
    "\n",
    "# --- Detailed Report for the Best Model ---\n",
    "print(f\"\\nClassification Report ({best_model_name}):\")\n",
    "print(classification_report(y_test, models[best_model_name], target_names=['Traveling', 'Foraging']))\n",
    "print(f\"\\nConfusion Matrix ({best_model_name}):\")\n",
    "print(confusion_matrix(y_test, models[best_model_name]))\n",
    "\n",
    "# --- Feature Importance for the Best GBM Model ---\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': best_lgbm.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(f\"\\nTop 10 Features:\")\n",
    "print(feature_importance.head(10))\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE MODELS\n",
    "# =============================================================================\n",
    "\n",
    "# Save the best individual model and the scaler\n",
    "joblib.dump(best_lgbm, 'best_shark_model_lightgbm.pkl')\n",
    "joblib.dump(scaler, 'shark_model_scaler.pkl')\n",
    "print(\"\\nBest model saved as best_shark_model_lightgbm.pkl\")\n",
    "print(\"Scaler saved as shark_model_scaler.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
