{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd8dd685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Set up device (use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86c3a607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 8996 sequences of length 10.\n"
     ]
    }
   ],
   "source": [
    "# Load the data that includes the 'id' and 'date' columns\n",
    "df = pd.read_csv('processed_shark_data.csv')\n",
    "\n",
    "# Define feature columns to use\n",
    "feature_columns = [\n",
    "    'lat', 'lon', 'month', 'hour', \n",
    "    'speed_avg_roll', 'speed_std_roll', 'angle_avg_roll', 'angle_std_roll'\n",
    "]\n",
    "target_column = 'is_foraging'\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
    "joblib.dump(scaler, 'pytorch_scaler.pkl') # Save the scaler\n",
    "\n",
    "# Function to create sequences for each shark\n",
    "def create_sequences(df, sequence_length=10):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    # Group by shark ID to create sequences only within a single track\n",
    "    for shark_id, group in df.groupby('id'):\n",
    "        features = group[feature_columns].values\n",
    "        labels = group[target_column].values\n",
    "        \n",
    "        if len(group) < sequence_length:\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(features) - sequence_length + 1):\n",
    "            seq = features[i:i + sequence_length]\n",
    "            target = labels[i + sequence_length - 1]\n",
    "            sequences.append(seq)\n",
    "            targets.append(target)\n",
    "            \n",
    "    return np.array(sequences), np.array(targets)\n",
    "\n",
    "# Create the sequences\n",
    "X_sequences, y_sequences = create_sequences(df, sequence_length=10)\n",
    "print(f\"Created {len(X_sequences)} sequences of length {X_sequences.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae7c11e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharkDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ce8d080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders created with batch size 64.\n"
     ]
    }
   ],
   "source": [
    "# Split the sequences into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sequences, y_sequences, test_size=0.2, random_state=42, stratify=y_sequences\n",
    ")\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SharkDataset(X_train, y_train)\n",
    "test_dataset = SharkDataset(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"DataLoaders created with batch size {batch_size}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2490de25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharkLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=64, num_layers=2, dropout=0.3):\n",
    "        super(SharkLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True, # Important!\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_size, 1) # Output a single value\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # LSTM returns output and hidden states\n",
    "        # We only need the output from the last time step\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        out = self.dropout(last_time_step_out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821ae7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch [5/50], Loss: 0.6030\n",
      "Epoch [10/50], Loss: 0.5955\n",
      "Epoch [15/50], Loss: 0.5853\n",
      "Epoch [20/50], Loss: 0.5770\n",
      "Epoch [25/50], Loss: 0.5662\n",
      "Epoch [30/50], Loss: 0.5548\n",
      "Epoch [35/50], Loss: 0.5358\n",
      "Epoch [40/50], Loss: 0.5176\n",
      "Epoch [45/50], Loss: 0.4988\n",
      "Epoch [50/50], Loss: 0.4657\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_size = X_train.shape[2] # Number of features\n",
    "model = SharkLSTM(input_size=input_size).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# BCEWithLogitsLoss is numerically stable and includes the sigmoid activation\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(epochs):\n",
    "    model.train() # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    for sequences, targets in train_loader:\n",
    "        # Move data to the configured device\n",
    "        sequences, targets = sequences.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(sequences).squeeze()\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ced2304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Model Evaluation ---\n",
      "Model Accuracy: 66.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Traveling       0.73      0.82      0.77      1264\n",
      "    Foraging       0.40      0.28      0.33       536\n",
      "\n",
      "    accuracy                           0.66      1800\n",
      "   macro avg       0.56      0.55      0.55      1800\n",
      "weighted avg       0.63      0.66      0.64      1800\n",
      "\n",
      "\n",
      "PyTorch model saved as 'shark_pytorch_lstm_model.pth'\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Set the model to evaluation mode\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad(): # No need to calculate gradients for evaluation\n",
    "    for sequences, targets in test_loader:\n",
    "        sequences = sequences.to(device)\n",
    "        outputs = model(sequences).squeeze()\n",
    "        \n",
    "        # Apply sigmoid to get probabilities, then threshold at 0.5 for class labels\n",
    "        preds = (torch.sigmoid(outputs) >= 0.5).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(targets.numpy())\n",
    "\n",
    "# Calculate and print the final results\n",
    "accuracy = (np.array(all_preds) == np.array(all_targets)).mean()\n",
    "print(f\"\\n--- Final Model Evaluation ---\")\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(all_targets, all_preds, target_names=['Traveling', 'Foraging']))\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), 'shark_pytorch_lstm_model.pth')\n",
    "print(\"\\nPyTorch model saved as 'shark_pytorch_lstm_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97e4c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
